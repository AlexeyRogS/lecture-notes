\documentclass[../real_analysis.tex]{subfiles}

\begin{document}

    \section{Series}\label{sec:series}
        Outside the world of mathematics the words `sequence' and `series' can often mean the same thing, however, within the context of real analysis this is not the case. We define a series to be an infinite summation, which we can think of as a sum of all of the terms in a sequence. For example, we can take any sequence $(x_n)_n$ and form the series
        \begin{equation}
            \sum_{n=1}^\infty x_n=x_1+x_2+x_3+\dots.
        \end{equation}
        But when is it valid to write that a series has some finite value? If we add up infinitely many things there will be many cases where the sum is infinite, for example a series constructed from any constant sequence would just be that constant added up infinitely many times, which obviously has no finite value. To solve this problem, we introduce a new tool: the sequence of partial sums.
        \begin{definition}
            Let $S$ be a series $\sum_{n=1}^\infty x_n$ generated by some sequence $(x_n)_n$. Then we define the $n$\ts{th} \textbf{partial sum} of $S$ as
            \begin{equation}
                S_n=\sum_{k=1}^n=x_1+x_2+\dots+x_n.
            \end{equation}
        \end{definition}

        \subsection{Convergence of Series}\label{subsec:convergence-of-series}
            It is clear that the $n$\ts{th} partial sums always have a finite value (if the elements of the sequence are well-defined) since they are only finite sums. This means we can consider the partial sums as a sequence in their own right $(S_n)_n$, and we can say that if the sequence of partial sums converges to some value, then the infinite series takes on that exact value.
            \begin{definition}
                Let $S$ be a series $\sum_{n=1}^\infty x_n$. Then we say $S$ \textbf{converges} if $(S_n)_n$ converges, in which case
                \begin{equation}
                    S=\sum_{n=1}^\infty x_n=\lim_{n\to\infty}S_n.
                \end{equation}
                If $(S_n)_n$ diverges, then $\sum_{n=1}^\infty$ diverges and $S$ is either not defined, or we can say that $S=\infty$ or $S=-\infty$, depending on the nature of the divergence of $(S_n)_n$.
            \end{definition}
            \begin{example}
                Consider the sequence given by $x_n=\frac{1}{n(n+1)}$. The sequence of partial sums of the series generated by this sequence is given by
                \begin{align}
                    S_n&=\sum_{k=1}^n\frac{1}{k(k+1)}\\
                    &=\frac{1}{1\cdot}+\frac{1}{2\cdot3}+\frac{1}{3\cdot4}+\dots+\frac{1}{n(n+1)}\\
                    &=\left(\frac{1}{1}-\frac{1}{2}\right)+\left(\frac{1}{2}-\frac{1}{3}\right)+\left(\frac{1}{3}-\frac{1}{4}\right)+\dots+\left(\frac{1}{n}-\frac{1}{n+1}\right)\\
                    &=1-\frac{1}{n+1}.
                \end{align}
                We can see that $(S_n)_n$ converges to 1, and hence $\sum_{n=1}^\infty\frac{1}{n(n+1)}$ is a convergent series with the value 1.
            \end{example}
            \begin{example}
                Consider the seqeuence given by $x_n=(-1)^n$. Then
                \begin{equation}
                    S_1=-1,\ S_2=-1+(-1)^2=0,\ S_3=-1+(-1)^2+(-1)^3=-1\dots
                \end{equation}
                Thus the $n$th partial sum is given by
                \begin{equation}
                    S_n=\begin{cases}
                        -1 & \text{if $n$ is odd}\\
                        0 & \text{if $n$ is even}
                    \end{cases}
                \end{equation}
                This sequence $(S_n)_n$ is divergent, and so the series $\sum_{n=1}^\infty(-1)^n$ is not defined.
            \end{example}

        \subsection{Convergence of the Generating Sequence}\label{subsec:convergence-of-the-generating-sequence}
            In the first example we can see that the sequence which generated the convergent series converged to 0 (not the partial sums!). This turns out to be a general fact which we will now show.
            \begin{theorem}\label{thm:seq-of-series-to-0}
                Let $S=\sum_{n=1}^\infty x_n$ be a convergent series. Then $x_n\to0$ as $n\to\infty$.
            \end{theorem}
            \begin{proof}
                Since $S$ is convergent, the sequence $(S_n)_n$ converges. Note that $x_n=S_n-S_{n-1}$ for all $n\geq2$, and since both $S_n\to S$ and $S_{n-1}\to S$ as $n\to\infty$, we have
                \begin{equation}
                    x_n\to S-S=0\ \text{as}\ n\to\infty.
                \end{equation}
            \end{proof}
            It makes sense that this should be the case. If the generating sequence did not tend to 0, then we would be adding together infinitely many nonzero terms, and it would be very strange if this somehow converges! It would be useful if the converse of this theorem was also true ($x_n\to0\implies\sum_{n=1}^\infty$ converges), but this turns out not to be the case and leads to one of the most (in)famous counterexamples in mathematics.
            \begin{example}[The Harmonic Series]
                The series $\sum_{n=1}^\infty\frac{1}{n}$ diverges to $+\infty$.\\
                Fist note that $(S_n)_n$ is an increasing sequence since $S_n-S_{n-1}=\left(\sum_{k=1}^n\frac{1}{k}\right)-\left(\sum_{k=1}^{n-1}\frac{1}{k}\right)=\frac{1}{n}\geq0$. Now consider the subsequence $(S_{2^m})_m$. The general term of this sequence is given by
                \begin{align}
                    S_{2^m}=&1+\frac{1}{2}+\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+\frac{1}{8}\right)+\left(\frac{1}{9}+\dots+\frac{1}{16}\right)\\
                    &+\dots+\left(\frac{1}{2^{m-1}+1}+\dots+\frac{1}{2^m}\right),
                \end{align}
                where the brackets indicate how the terms are added on in each partial sum. The $i$th bracketed expression in each term is given by
                \begin{equation}
                    t_i=\frac{1}{2^{i-1}+1}+\frac{1}{2^{i-1}+2}+\dots+\frac{1}{2^i}.
                \end{equation}
                There are $2^i-2^{i-1}$ values in this sum, and they are all greater than or equal to $\frac{1}{2^i}$, so $t_i\geq2^{i-1}\frac{1}{2^i}=\frac{1}{2}$. Hence
                \begin{equation}
                    S_{2^m}=1+t_1+t_2+\dots+t_m\geq1+\frac{m}{2}.
                \end{equation}
                Now, let $K\in\mathbb{N}$, then for $n\geq2^{2K}$ we have
                \begin{equation}
                    S_n\geq S_{2^{2K}}\geq 1+\frac{2K}{2}=1+K>K,
                \end{equation}
                and so since the sequence of partials sums diverges to $\infty$, the series must also diverge.
            \end{example}

        \subsection{Geometric Series}\label{subsec:geometric-series}
            \begin{definition}
                A \textbf{geometric series} is an infinite series of terms which have a constant ratio between them. They can be expressed as
                \begin{equation}
                    \sum_{n=1}^\infty ar^{n-1}
                \end{equation}
                for some $a\in\mathbb{R}$ and $r\in\mathbb{R}\setminus\{0\}$.
            \end{definition}
            \begin{theorem}\label{thm:geometric-series}
                The geometric series $\sum_{n=1}^\infty ar^{n-1}$ converges if and only if $\abs{r}<1$, and in this case $\sum_{n=1}^\infty ar^{n-1}=\frac{a}{1-r}$.
            \end{theorem}
            \begin{proof}
                First note the general term of the sequence of partial sums:
                % TODO: show this
                \begin{equation}
                    S_n=\sum_{k=1}^n ar^{k-1}=a+ar+ar^2+\dots+ar^{k-1}=\frac{a(1-r^n)}{1-r}\tag{$r\neq1$}. %TODO: show
                \end{equation}
                If $\abs{r}<1$, then by theorem \ref{thm:std-seq} $r^n\to0$ as $n\to\infty$. Hence $S_n\to\frac{a}{1-r}$. If $r=1$, then $S_n=an$ which tends to $\pm\infty$ depending on the sign of $a$. If $r=-1$, then $S_n$ is 0 if $n$ is even and $a$ if $n$ is odd, hence $S_n$ diverges. It can be shown that if $\abs{r}>1$, $S_n$ diverges, hence all the cases are covered and the only way the series can converge is if $\abs{r}<1$.
            \end{proof}

    \section{The Comparison Test}\label{sec:comparison-test}
        What might have become clear is that determining convergence of series is more complicated than for sequences because the terms themselves are more complicated. Because of this,we want to create tools which will allow us to more easily deduce the properties of a series. One of these is the comparison test, which allows us to determine if a series is convergent by comparing it to a known series such as $\sum_{n=1}^\infty\frac{1}{n}$ or $\sum_{n=1}^\infty\frac{1}{n^2}$.
        \begin{theorem}[Comparison Test]\label{thm:comparison-test}
            Let $(x_n)_n$ and $(a_n)_n$ be sequence with no negative terms. Then
            \begin{enumerate}[label={\upshape(\roman*)}]
                \item If $\sum_{n=1}^\infty a_n$ is convergent and $\exists N\in\mathbb{N}$ such that $x_n\leq a_n\ \forall n\geq N$, then $\sum_{n=1}^\infty x_n$ also converges.
                \item If $\sum_{n=1}^\infty a_n$ is divergent and $\exists N\in\mathbb{N}$ such that $x_n\geq a_n\ \forall n\geq N$, then $\sum_{n=1}^\infty x_n$ also diverges.
            \end{enumerate}
        \end{theorem}
        \begin{proof}
            \begin{enumerate}[label={\upshape(\roman*)}]
                \item Let $A=\sum_{n=1}^\infty a_n$ and note that since all the terms $x_n$ are positive, $S_n=\sum_{k=1}^n x_k$ is an increasing sequence. Then, since there exists some integer $N$ such that for all $k\geq N$ we have $x_n\leq a_n$, we get
                \begin{equation}
                    \sum_{k=N}^n x_k\leq\sum_{k=N}^n a_k\leq\sum_{k=1}^\infty a_k=A.
                \end{equation}
                Therefore, for all $n\geq1$ we have
                \begin{align}
                    S_n&=a_1+a_2+\dots+a_{N-1}+\sum_{k=N}^n x_k\\
                    &\leq a_1+a_2+\dots+a_{N-1}+A,
                \end{align}
                and hence $S_n$ is bounded above. But now $S_n$ is both monotone and bounded and so by the MCT (\ref{thm:MCT}), $S_n$ must be convergent and thus the series $\sum_{n=1}^\infty x_n$ converges.
                \item Suppose by way of contradiction that $\sum_{n=1}^\infty x_n$ is convergent, then following the argument in part (i) implies that $\sum_{n=1}^\infty a_n$ is convergent, which is a contradiction, meaning $\sum_{n=1}^\infty x_n$ must be divergent.
            \end{enumerate}
        \end{proof}
        Note that we can also multiply the terms of the generating sequence by a positive constant, as this does not affect the convergence of the series, i.e. we can check that $\exists N\in\mathbb{N}, c>0$ such that $x_n\leq ca_n$ or $x_n\geq ca_n\ \forall n\geq N$.

        We using the comparison test, often the most convenient series to compare to are the so-called `$p$-series', that is, those of the form $\sum_{n=1}^\infty\frac{1}{n^p}$.
        \begin{example}
            Let $p\in\mathbb{R}$. Then the series $\sum_{n=1}^\infty\frac{1}{n^p}$ converges if and only if $p>1$.\\
            Suppose $p\geq2$. Then $\frac{1}{n^p}\leq\frac{1}{n^2}$ for all $n\in\mathbb{N}$, so $\sum_{n=1}^\infty\frac{1}{n^p}$ is convergent by the comparison test.\\
            Now suppose $p\leq1$. Then $\frac{1}{n^p}\geq\frac{1}{n}$ for all $n\in\mathbb{N}$, so $\sum_{n=1}^\infty\frac{1}{n^p}$ is divergent by the comparison test.\\
            In the case of $p\in(1, 2)$, the series $\sum_{n=1}^\infty\frac{1}{n^p}$ can be proved to be convergent by the `integral test'.
        \end{example}

        \subsection{The Ratio Test}\label{subsec:ratio-test}
            % TODO: move this to its own section or rename current section?
            \begin{theorem}[Ratio Test]
                Let $(x_n)_n$ be a sequence of positive terms.
                \begin{enumerate}[label={\upshape(\roman*)}]
                    \item If $\exists N\in\mathbb{N}$ and $r<1$ such that for all $n\geq N, \frac{x_{n+1}}{x_n}\leq r$, then $\sum_{n=1}^\infty x_n$ is convergent.
                    \item If $\exists N\in\mathbb{N}$ and $r>1$ such that for all $n\geq N, \frac{x_{n+1}}{x_n}\geq r$, then $\sum_{n=1}^\infty x_n$ is divergent.
                    \end{enumerate}
            \end{theorem}
            \begin{proof}
                \begin{enumerate}[label={\upshape(\roman*)}]
                    \item First note that $x_{N+1}\leq rx_N$, $x_{N+2}\leq rx_{N+1}=r^2x_N$, etc. and so $x_{N+k}\leq r^kx_N$. Now consider the sequence given by
                    \begin{equation}
                        a_n=r^{n-N}x_N=(x_Nr^{-N})\cdot r^n.
                    \end{equation}
                    This is a geometric series which is convergent by theorem \ref{thm:geometric-series}. Then notice that for all $n\geq N$ we have
                    \begin{equation}
                        x_n=x_{N+(n-N)}\leq r^{n-N}x_N=a_n,
                    \end{equation}
                    and hence by the comparison test $\sum_{n=1}^\infty x_n$ is convergent.
                    %TODO: part 2 proof
                \end{enumerate}
            \end{proof}
            In effect, what the ratio test says is that if $\lim_{n\to\infty}\frac{x_{x+1}}{x_n}$ exists and is less than 1, then the series converges and if it is greater than 1 then the series diverges. If the limit is equal to 1, then we cannot determine the behaviour of the series from the ratio test. The ratio test works best for geometric series because of the cancellation that happens when dividing the exponents. It also works well for some series containing factorials of $n$ for the same reason.
            \begin{example}
                Show that the series $\sum_{n=1}^\infty\frac{2^n}{n!}$ converges.\\
                The generating sequence is given by $x_n=\frac{2^n}{n!}$, so
                \begin{equation}
                    \frac{x_{n+1}}{x_n}=\frac{\left(\frac{2^{n+1}}{(n+1)!}\right)}{\left(\frac{2^n}{n!}\right)}=\frac{2^{n+1}n!}{2^n(n+1)!}=\frac{2}{n+1}\to0\text{ as }n\to\infty.
                \end{equation}
                Hence by the ratio test, the series converges.
            \end{example}

    \section{Series of Positive \& Negative Terms}\label{sec:series-of-positive-and-negative-terms}
        Up to this point we have only looked at methods for analysing the behaviour of series generated by sequences which only contain positive terms. This is very limiting and we will now expand the scope to general sequences.
        \begin{definition}
            A series $\sum_{n=1}^\infty x_n$ is called \textbf{absolutely convergent} if $\sum_{n=1}^\infty\abs{x_n}$ converges.
        \end{definition}
        \begin{theorem}
            Every absolutely convergent series is also convergent.
        \end{theorem}
        \begin{proof}
            Consider an absolutely convergent series $\sum_{n=1}^\infty x_n$. Note that we have
            \begin{equation}
                x_n+\abs{x_n}\leq2\abs{x_n},\ \forall n\in\mathbb{N}.
            \end{equation}
            Thus, by the comparison test (comparison with $\sum_{n=1}^\infty\abs{x_n}$), the series $\sum_{n=1}^\infty(x_n+\abs{x_n})$ is convergent. Now, we can write
            \begin{equation}
                \sum_{n=1}^\infty x_n=\sum_{n=1}^\infty(x_n+\abs{x_n})-\sum_{n=1}^\infty\abs{x_n},
            \end{equation}
            and since $\sum_{n=1}^\infty x_n$ is the different of two convergent series, it is also convergent (follows from theorem \ref{thm:seq-lim-props}).
        \end{proof}
        \begin{example}
            Show that the series $\sum_{n=1}^\infty\frac{(-1)^n}{n^2}$ is convergent.\\
            Note that since $\abs*{\frac{(-1)^n}{n^2}}=\frac{1}{n^2}$, by comparison with the series $\sum_{n=1}^\infty\frac{1}{n^2}$ the series $\sum_{n=1}^\infty\abs*{\frac{(-1)^2}{n^2}}$ is convergent. Hence the series $\sum_{n=1}^\infty\frac{(-1)^2}{n^2}$ is absolutely convergent and therefore convergent.
        \end{example}
        We will now look at two more convergence tests which will be useful.

        \subsection{The Root Test}\label{subsec:root-test}
            \begin{theorem}[The Root Test]\label{thm:root-test}
                Let $\sum_{n=1}^\infty x_n$ be a series.
                \begin{enumerate}[label={\upshape(\roman*)}]
                    \item If $\exists N\in\mathbb{N}$ and $r<1$ such that for all $n\geq N, \abs{x_n}^\frac{1}{n}\leq r$, then the series converges.
                    \item If $\exists N\in\mathbb{N}$ and $r>1$ such that for all $n\geq N, \abs{x_n}^\frac{1}{n}\geq r$, then the series diverges.
                \end{enumerate}
            \end{theorem}
            \begin{proof}
                \begin{enumerate}[label={\upshape(\roman*)}]
                    \item For $n\geq N$ we have $\abs{x_n}^\frac{1}{n}\leq r\implies\abs{x_n}\leq r^n$. Let $a_n=r^n$, then $(a_n)_n$ generates a convergent geometric series so by the comparison test the series $\sum_{n=1}^\infty\abs{x_n}$ converges. Thus the series $\sum_{n=1}^\infty x_n$ is absolutely converges and therefore converges.
                    \item For all $n\geq N$ we have $\abs{x_n}^\frac{1}{n}\geq r\implies\abs{x_n}\geq r^n$, so $x_n$ must be a divergent sequence ($\pm\infty$). In particular, $x_n$ does not converge to 0 as $n\to\infty$, so by the contrapositive of theorem \ref{thm:seq-of-series-to-0}, $\sum_{n=1}^\infty x_n$ diverges.
                \end{enumerate}
            \end{proof}
            As with the ratio test, the root test says is that if $\lim_{n\to\infty}\abs{x_n}^\frac{1}{n}$ exists and is less than 1, then the series converges, if it is greater than 1 then the series diverges and if it is equal to 1 then the root test gives us no information.
            
        \subsection{The Alternating Series Test}\label{subsec:alternating-series-test}
            \begin{theorem}[Alternating Series Test]
                Let $(x_n)_n$ be a positive decreasing sequence with limit 0 ($\lim_{n\to\infty}x_n=0$). Then
                \begin{equation}
                    \sum_{n=1}^\infty (-1)^nx_n
                \end{equation}
                is a convergent series.
            \end{theorem}
            \begin{proof}
                % TODO: prove
            \end{proof}
            \begin{example}
                The sequence given by $x_n=\frac{1}{n}$ is positive and decreasing with limit 0. Thus by the alternating series test, $\sum_{n=1}^\infty\frac{(-1)^n}{n}$ is convergent. Note that it is not absolutely convergent since $\abs*{\frac{(-1)^n}{n}}=\frac{1}{n}$ generates the harmonic series which diverges.
            \end{example}
            % TODO: move this?
            \begin{theorem}\label{thm:root-test-tools}
                These are some handy facts to keep in mind when using the root test.
                \begin{enumerate}[label={\upshape(\roman*)}]
                    \item $\lim_{n\to\infty}n^\frac{1}{n}=1$.
                    \item If $r>0$, then $\lim_{n\to\infty}r^\frac{1}{n}=1$.
                    \item Suppose $(a_n)_n$ is a positive sequence and $\frac{a_{n+1}}{a_n}\to r$, then $a_n^\frac{1}{n}\to r$.
                \end{enumerate}
            \end{theorem}
            \begin{proof}
                % TODO: prove
            \end{proof}

    \section{Power Series}\label{sec:power-series}
        A power series is a series of the powers of some unknown variable, basically an infinite polynomial. We make a clearer definition here.
        \begin{definition}
            Let $(a_n)_n$ be a sequence of real numbers. Then a power series in some variable $x$ is given by
            \begin{equation}
                \sum_{n=1}^\infty a_nx^n.
            \end{equation}
        \end{definition}
        If we compare the form of a power series to a geometric series, we can see that the convergence of a power series depends on the value of the variable $x$, so the power series is a function of $x$. In particular, it can be shown that a power series either converges for all $x\in\mathbb{R}$, or it converges in some interval of the real line symmetric about $x=0$. For example, if $(a_n)_n$ is the constant sequence $(1)_n$, then the power series $\sum_{n=1}^\infty x^n$ converges for $-1<x<1$.
        \begin{theorem}
            Consider a power series $\sum_{n=1}^\infty a_nx^n$ and suppose $\lim_{n\to\infty}\abs{a_n}^\frac{1}{n}=\beta$. Then, taking $R=\frac{1}{\beta}$ (if $\beta=0$, then take $R=\infty$; if $\beta=\infty$, then take $R=0$), we have
            \begin{enumerate}[label={\upshape(\roman*)}]
                \item The power series converges for $x\in(-R, R)$.
                \item The power series diverges for $x\notin(-R, R)$.
            \end{enumerate}
            $R$ is called the \textbf{radius of convergence} of the power series.
        \end{theorem}
        \begin{proof}
            Fix $x\in\mathbb{R}$. Then
            \begin{equation}
                \abs{a_nx^n}^\frac{1}{n}=\abs{a_n}^\frac{1}{n}\abs{x}\to\beta\abs{x}\text{ as }n\to\infty.
            \end{equation}
            \begin{enumerate}[label={\upshape(\roman*)}]
                \item\begin{enumerate}[label={\upshape(\alph*)}]
                    \item $\beta=0$ (so $R=\infty$ and $\abs{x}<R$) implies $\beta\abs{x}<1$, so by the root test $\sum_{n=1}^\infty a_nx^n$ converges.
                    \item $\beta>0$ and $\beta\abs{x}<1$ (so $\abs{x}<R$). By the root test $\sum_{n=1}^\infty a_nx^n$ converges.
                \end{enumerate}
                \item\begin{enumerate}[label={\upshape(\alph*)}]
                    \item $\beta=\infty$ (so $R=0$). Then the root test implies that $\sum_{n=1}^\infty a_nx^n$ diverges.
                    \item $\beta>0$ and $\beta\abs{x}>1$ (so $\abs{x}>R$). Then by the root test $\sum_{n=1}^\infty a_nx^n$ diverges.
                \end{enumerate}
            \end{enumerate}
        \end{proof}
        Note that when we are finding $R$, it can often be much easier to find the limit $\lim_{n\to\infty}\abs*{\frac{a_{n+1}}{a_n}}$ than $\lim_{n\to\infty}\abs{a_n}^\frac{1}{n}$, and theorem \ref{thm:root-test-tools} (iii) says that they have the same value.
        \begin{example}
            For what values of $x$ does the power series $\sum_{n=1}^\infty\frac{n+1}{2^n}x^n$ converge?\\
            Using the root test, we see that
            \begin{equation}
                \abs{a_n}^\frac{1}{n}=\left(\frac{n+1}{2^n}\right)^\frac{1}{n}=\frac{(n+1)^\frac{1}{n}}{2}\to\frac{1}{2}\text{ as }n\to\infty.
            \end{equation}
            So the radius of convergence of this power series is $R=2$. What happens for the bounding values? For $x=2$, the power series becomes $\sum_{n=1}^\infty\frac{n+1}{2^n}2^n=\sum_{n=1}^\infty(n+1)$ which is divergent. For $x=-2$, the power series is $\sum_{n=1}^\infty\frac{n+1}{2^n}(-2)^n=\sum_{n=1}^\infty(-1)^n(n+1)$ which is divergent. %TODO: show
            Hence the power series converges $\forall x\in(-2, 2)$.
        \end{example}
        \begin{example}
            For what values of $x$ does the power series $\sum_{n=1}^\infty\frac{1}{n!}x^n$ converge?\\
            Inspecting the ratio of consecutive terms, we see
            \begin{equation}
                \frac{\abs{a_{n+1}}}{\abs{a_n}}=\frac{\frac{1}{(n+1)!}}{\frac{1}{n!}}=\frac{n!}{(n+1)!}=\frac{1}{n+1}\to0.
            \end{equation}
            Hence $\beta=0$ and $R=\infty$, so the power series converges $\forall x\in\mathbb{R}$.
        \end{example}

\end{document}
